The First cycle resolves on loading vertices from the file and synchronizing them across peers. There for each peer gets assign a key range which is based on the Java \textit{hashCode()} of the key object and its modulo based on the peers position in the starting list. 
The architecture of the application consist out of multiple steps. The first steps is to setup our environment,  this includes calling the \texttt{JobRegistrationApp} to register all needed jobs on all peers and also validating the passed in parameters. Some parameters are required like the file(s) to load and the format, other parameters are optional like the amount of peers or an list of peers to run this application on, by default this application will try to use every peer available for maximizing the loading speed.\\


That is the reason for the distribution of the loading process. The loading from disk is much more lightweight then the parsing of the data itself.  Both times seem to be linear and as result growing at a constant rate. On a single machine, supposed it could handle the amount of data in RAM, huge data sets of multiple gigabytes would take a long time to load. To accelerate this process. The file get split up into smaller chunks then distributed onto many peers and loaded on the peer with multiple threads.
