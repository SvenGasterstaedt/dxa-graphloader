\chapter{Architecture}
\label{Architecture}
This chapter will give a brief introduction into DXRAM. Then the schema how the graphs are going to be loaded will be explained. After that problems that have occured are discussed. 
\section{DXRAM}
DXRAM is a distrubuted in-memory key/value-store. 
It is implemented in Java and optimized to manage billions of small data objects. 
For low latency data access DXRAM keeps 100\% of the data in RAM.
DXRAM provides a low data overhead, which suits graph based applications.
Nodes of DXRAM can take the role of a "normal" peer or a superpeer. Superpeers are arranged in a chord like ring structure (Fig.~\ref{topology}). 
\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\linewidth]{img/topology.png}
	\caption{Topology of DXRAM}
	\label{topology}
\end{figure}
Peers are always assigned to one superpeer (Fig. ~\ref{topology}). Superpeer take over administrative tasks within the distributed system while peers serve the role of storage and/or backup nodes.
DXRAM provides its own application programming interface through the following services. DXRAM uses a distributed file system, so every node can access every file. To access or start an application within DXRAM, its Jar file must be located in the "plugin" folder. Then it needs to be  configured to autostart in the configurations of DXRAM or started via the terminal application.\\
Applications get access through services to various different feature of the system, like access on the distributed in-memory key/value store, the ability to run jobs and task on specified peers or to invoke other applications.\\

\noindent{\textit{BootService}}\\
The BootService provides access to all nodes inside the system. It can retrieve the specified roles of each node and its unique id.\\\\
\noindent{\textit{ChunkService}}\\
The ChunkService provides access to the in-memory key/value store of DXRAM. It allows to create, get, put and remove Chunks. Chunks are data structures, which extend the \textit{AbstractChunk} class. Each chunk must specify its size in bytes and how its data gets serialized and deserialized to binary. To identify chunks they get assigned an unique id, which links them to a location of their current size in the key/value store. If their size changes, their linked key/value store size must be changed as well, so their current size can fit into the storage. There are multiple implementations of ChunkServices. One of them is the regular ChunkServices, who allows storing chunks on every storage peer of the network, but there is also ChunkLocalService, who stores every chunk on the executing peer.\\\\
\noindent{\textit{NameserviceService}}\\
The NameserviceService allows to assign a unique \textit{short} String key to a chunk and register its key globally, so if other peers request a chunk id for this key they get the linked id. This is useful for getting chunks which do not change their content, if they are needed on multiple peers.\\\\
\noindent{\textit{SychronizationService}}\\
The SychronizationService allows to register barriers on peers. Barriers are used to control the flow of an application on multiple peers. They can also be used to share data structures between peers. A barrier gets created, stored and assigned an unique string key under which it gets registered at the NameserviceService. Then the barrier can be loaded on every needed peer and peers can sign on the barrier with the SychronizationService. If a peer signs on a barrier, it signals that it reached this point of the code and can optionally wait for the time critical tasks to end on all other peers. If all peers reached this point, they get notified and can continue.\\\\
\newpage
\subsection{Integration in DXRAM}
There are four main methods of integrating foreign code into DXRAM. There are application, jobs, task and functions. Applications are started through an autostart entry in the DXRAM configuration file or via the DXA-Terminal[?] application. There can only be a single instance of an application per peer. Applications are a good method of starting foreign code in DXRAM due to the lacking possibilities of starting jobs, task and functions, which is intended.\\
Jobs are a method to run foreign code on local and remote peers. Each peer got a local job queue and a pool of workers, who execute jobs of this queue based on a work stealing approach. Jobs do not offer a way of starting them without an application or an other method invoking them. For remote deployment, jobs must be able to be serialized.\\
The next approach are tasks. Task run on a computing group, which can be accessed through the \textit{MasterSlaveService}. It is lead by a peer, who is the master of this group. The master coordinates the whole computing group and takes over administrative tasks. Tasks can only be run on computing groups. This makes them more inflexible then jobs, which can be executed on any peer without setting up computing groups.\\
The last method is a distributed function, these functions get send to remote peers with their parameters and then their computation results gets returned to the invoking methods.
\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\linewidth]{img/codeintegration.png}
	\caption{Schema of Foreign Code Execution in DXRAM}
	\label{codeintegration}
\end{figure}

As shown in Fig.~\ref{codeintegration} applications are needed to execute jobs, tasks and functions. For the graph loader application jobs must be used to utilize multiple peers, as to the fact that it is too much effort to set up a computing group. Also graph loading is an operation, which is probably executed once. The graph loader must be a job to provide the possibility to integrate the graph loader into other projects. For direct access within DXRAM the graph loader job will be wrapped by an application. This can be seen in Fig.~\ref{integration}. This allows other projects to start the graph loader as job and avoiding application chaining. Due to the fact that jobs can invoke each other, exists the possibility to start multiple jobs on local and remote peers.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\linewidth]{img/schema.png}
	\caption{Schema of Integeration of the Graph Loader in DXRAM}
	\label{integration}
\end{figure}
\newpage
\section{Architecture of the Application}

There are two main task which could use speeding up. The first task is reading the ifle from disk and second is parsing the file and storing its data. A single machine will have much faster read speed from disk, due to fact that no data transmission between peers is needed. Therefore the loading into memory and distribution inside the distributed system, maybe slower then on a single machine. But this reading speed is depend on many factors, like the hardware and the implementation of the input methods. The main target is to speed up the performance of the parsing and storing of the graph, while equally distributing it into the in-memory system. As show in Fig.~\ref{loadingparsing} is the impact from parsing approximately 10 times higher then the impact from reading. Parsing the file is a computing intense task and suits the design of a distributed system.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\linewidth]{img/tasktime.png}
	\caption{Local Single Threaded Benchmark of Loading and Parsing of different sized Graphs in Edge List Format for a Single-Pass Step}
	\label{loadingparsing}
\end{figure}

\subsection{Data Structures}
Sharing data structures between peers was not as straight forward as expected. Duo to the fact that the size of the graphs and their data structure grew at the same ratio. This lead to the problem of time outs of the networking component, when storing these structures in chunks on remote peers. To address this problem an layer of indirection was introduced, to divide the big chunks into smaller sections. Therefore, two dimensional arrays where needed. 
Also a wrapper class for the graph was need, it consist out of a set of vertex maps for each peer, which can also be stored in a chunk. The id of the graph object gets returned at the end of the graph loader, to provide access to following applications.\\
Apart from the chunk data structures, there were some important things to consider. Sections with data structures which get changed and accessed, should be thread safe for multiple job usage. There for \texttt{ConcurrentHashMaps} were used and \texttt{Lock}s to prevent jobs from interfering with each other. Also a queue was used for the chunks, which containing the file data, were removing elements also had to be thread safe.\\
For sending edges between peers, the wrapped maps could not be used, as maps are based on a unique key, which does not suit edges. Therefore, two arrays with linked entries were used to send edges between peers.\\
First linked lists were used for parallel reading chunks with file data were each chunk identified the next one. This approach was quite unstable, due to busy polling of the chunks on remote peers. The time gain of three seconds per gigabyte, did not make up the instabilities of busy polling on remote peers.\\

\subsection{Deployment of File data on nodes}
The distribution of the data inside the distributed system, will be done in three steps. The first step is to load the file into memory. DXRAM provides an network file system (NFS). So every peers could access and request files via network even if it is not physically on this peer. There are several methods of file interaction which could be used and need to be considered.

\subsubsection{Efficient Loading of File into Memory}
One key ability of the application is that the loading of the file system should be fast. Often the reading/writing from Storage Devices is a bottleneck. The first problem is that most Storage Devices are designed for sequential read and write operations and often support only one operation at a time. This leads to the problem that multiple threads reading the same storage device is a waste of performance and will not speed up the input rate of the file. Another problem is that accessing a storage device will cause a context switch. This results in the application stopping, while the kernel executes sensitive functions. This problem is often address with buffering the files and by requesting more data then needed in one step. Buffer sizes are one optimization problem to consider. [5]\\
The traditional way of accessing the data of a file is a common routine. First the file gets opened after that the data can be read, written in sequential or random order. This method causes many context switches, which force the application to stop, while the kernel executes sensitive functions. That is why in nearly every case IO gets buffered, so the context switches occur less often. [6]\\
This problem is addressed by memory mapping and suits large files where the context switches can be reduced drastically. A virtual memory mapping between the file system and the application address space is created. So expansive system calls can be avoided. The setup of this method is more expensive then the setup of a file reader. [5,6] But on the other hand mapped memory takes longer to setup and consumes more memory. Also Java does not offer the ability to free the mapped memory region which could lead into problems. For that reason traditional file concept will be used, so that the remaining memory map does not occupies storage space.

\subsubsection{Splitting and Distributing the File}
The deployment of the data can be done in multiple ways for this consideration network traffic gets ignored as it occurs in every implementation. Factors to consider are buffer sizes and other optimizations.\\

	\textit{Splitting into multiple smaller Files}\\
	One option would be to spit the huge into multiple files. This would result in maybe one file for each peer, which participates in our loading process, which can be access via the NFS and then be loaded. this approach simple to implement, but comes with some flaws. One down side is that the files gets read into memory then gets written in parts onto disk again, so it can be accessed on other peers. This would result in another reading/writing cycle and the NFS can not be controlled through the provided services API of DXRAM. Therefor, there could still be some problems with accessing a file located on the same storage device. Also the peers must read in the newly created small file again. This would result in $O(2n)$ for reading and writing and $O(\frac{n}{peers})$ on every peer for reading again.\\
	
	\textit{Splitting into small Chunks}\\
	This approach would generate memory chunks with our file data and distribute them via the \texttt{ChunkService}. There are two option chunks could be the size of $\frac{n}{peers}$ but this would result in the last peer waiting $O(n-\frac{n}{peers})$. This could be an big impact for many peers and very large files. To address that issue, a chunk size gets introduced  it can be small but variable to avoid splitting in contiguous regions. So the waiting gets reduced to $O(chunk \times peers)$ and they can start working as soon as possible. The total time taken to read the file is again $O(n)$ but writing to disk is not needed and the data is kept in memory, so reading it from disk is not needed either.
	While the chunks are created, the chunks could already be processed on the remote peers, so that the concurrency reduces the loading time.\\
	
	\textit{Reading via NFS}\\
	This approach would be slow and inefficient, due bottlenecks on the peer where the file is located and multiple peers causing random access patterns on the disk itself.\\

Splitting the file into chunks seems to be the best approach and will be featured in this work. This could also be optimized by starting to read vertices, while still distributing chunks onto other peers. Therefor the reading peer must approximate its own distribution, to process files.

\subsection{Processing/ Parsing of Data}
Each chunk will be reading separately and can not be depend on other chunks. This results in the fact that all data for interpreting this chunks must be provided by itself. Therefore, a parser must operate on each line of the format to interpret it and extract vertices or edges. Parser for other formats can be added easily by following the structure of the given ones.\\
Often a format is pretty similar to an other already implemented format, so the existing parser can be adjusted to suit a new format and be added to the loader application. Often parsers can be used to according ot other already implemented formats.
The processing/ parsing of the data is unique for each format. Some formats like the TGF need a a different parser in contrast to XML or JSON sytle formats. it can be assumed that the algorithm, which needs the data. Can not proceed without a complete dataset. Therefore most of the peers have no computing task, so that their computing power is available for loading the format. This results in the ability to create many heavy working threads to use all cores of the peers to their maximal capacity.

\subsection{Storing}
The extracted vertices, edges and optional metadata needs to be stored. For this task the \texttt{ChunkService} will be used to push the created structs into the key/value-store. ne problem of the distrubuted loading is that peers cannot wait on other peers to get information about vertices, because this would result in too much messages and network traffic for large datasets. This still leaves the problem of not synchronized vertex objects on different peers, which cant be written to the key/value-store because their information could be incomplete. 
\subsubsection{Duplicate node objects}
Some formats (especially in edge lists) nodes just get described by their edges. One problem is if two different peers create the same nodes by different edges, so they do not know if other peers already created a node object for specific nodes. As result they create a local node object with the information they got. In that case after reading the file in its entirety some merge of all hash maps and objects would need to be done. This would result in the problem that one assumption of this work is that one peer ca not keep all nodes and edges in memory, because there are too many objects.\\
Hash-Distribution of the Nodes while reading\\
As result of the fact of reorganizing the nodes after they have been loaded is very inefficient and does not suit our needs, the next approach would be to organize the nodes at loading/parsing-time.
One way could be to hash the labels/ids used in the graph. Note that the hashes created have no security constrains and do not need to be unique. This hash could be used to divide the nodes onto the peers. As result every peer would buffer all nodes, that do not belong to its range of hash values, and send the information he gathered to the according peers. This way we do not have to deal with duplicate nodes or merge. One problem of this approach is, that peers could end up with a chunk, that contains no nodes that hash to itself.\\
Hash-Distribution after reading\\
The peers could finish reading the nodes of its chunk and create their own versions in their key-value store. After reading and parsing is finished, they could like in first solution send the key-value-objects to the according nodes based on the hashes of their ids. The target peer then merges all versions of that node and stores the copy that has all information, the other duplicates get deleted. This could result in many objects for one node for each peer

\section{Issues}
A requirement is that the file is located inside one of our peers storage devices and is not part of our distributed system in any way. Loading files via an internet connection, will not be featured. Some start parameters are indispensable for this type of application, like the absolute file path, the format of the file or rather the parser that should be used and the number of peers to load the file.
Frist one peer should deal with the file, to distribute it to our others peers. The best thing would be, if the peer, who deals with file, would be the same peer where the file is located on. Otherwise the file transfer to the according peer would be a waste of time, especially for file sizes of several gigabytes. After the file chunks got deployed on the network, they could be parsed and loaded by the prior assigned peers into our DXRAM key-value store. [4]


First the file data gets loaded into small chunks which can be distrubted tFirst the file gets partially mapped into the memory, where it can be read by the master slave. The master slave first deals with the format specifications of the file, providing the division schema for splitting the file into chunks. chunks and will be dealing with the file format. Due to the fact that the data, is in memory, multiple thread can access the data parallel to accelerate processing. The processed data will be collected in chunks which will be deployed to the assigned slaves.
The support of infiniband would be increasing the performance drastically, due to spreading the data in the distributed system would be faster.
When the chunks arrive on the according slaves, they could immediately start processing their chunks.
 
Due to fact that some formats specify data of nodes in two or multiple places inside the file, it would be an interesting approach of using for example two memory maps of the same file in different sections. So, the chunk (buffer) can be filled with both information at the same time and the node information lie on all on one slave.

\newpage
The loading of graph formats consist out of a \textit{Two-Pass-Step} based approach for distributed systems. Therefor the application will be split up into mutiple loading stages, which will alternate between loading, creating and resolving cycles which can be seen in Fig. ~\ref{stages}.
\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\linewidth]{img/schema_jobs.png}
	\caption{Stages of the GraphLoader Job}
	\label{stages}
\end{figure}

\section{Application Programming Interface}
The loader application provides an simple API. To add own formats, the class needs to be added to the \texttt{SupportedFormats} class and extend the \texttt{GraphFormat} class, which provides information of the required splitting type of the format and, how it should be read.
\subsection{SupportedFormats}
The \texttt{SupportedFormats} class, is the lookup table for all formats and which loader and splitter to use. It provides three simple functionalities.
\begin{itemize}
 \item[] \textit{addFormat(String key, Class$<$? extends GraphFormat$>$ format)}\\
 This function adds a format to the supported formats by assigning a key to a class type. For example ''{edgellist}'' is the key for the class \texttt{EdgeListFormat}.
 \item[] \textit{getFormat(final String key, final String[] files)}\\
 This function returns the class file for a given format key.
 \item[] \textit{isSupported(String key)}\\
This methods returns if a given key is supported.
\end{itemize}

\subsection{GraphFormat}
To add a custom format, the class needs to extend the \texttt{GraphFormat} class. A custom format specifies a Splitter and a Parser.
%First, a basic graph implementation with vertices and edges must be provided, those should be extendable due to different graphs, vertex and edge requirements of each format and application.
%Then the graph file will be split by lines for mostly likly simple graph file formats or interpreted for more complex ones. Out of those file splits - file chunks will be created which will be deployed to the corresponding peer. The peer will run the loading job with the loader provided for the format. The file format will be specified by the user as paramter.

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=1.0\linewidth]{img/layout.png}
%	\caption{Topology of DXRAM}
%	\label{topology}
%\end{figure}